# 第3周 大模型架构与关键术语
| 每日任务 | 学习素材（可直接访问） | 学习时长 | 备注 |
|----------|--------------|----------|------|
| Day1：Decoder-only（GPT系列） | 1. GPT-3论文：https://arxiv.org/abs/2005.14165 <br> 2. 李宏毅GPT详解：https://www.bilibili.com/video/BV17p4y1s7wq/ | 2h | 理解自回归生成原理 |
| Day2：Encoder-only（BERT系列） | 1. BERT论文：https://arxiv.org/abs/1810.04805 <br> 2. Hugging Face BERT教程：https://huggingface.co/docs/transformers/model_doc/bert | 2h | 掌握掩码语言模型任务 |
| Day3：Encoder-Decoder（T5系列） | 1. T5论文：https://arxiv.org/abs/1910.10683 <br> 2. 翻译任务Demo代码：https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/run_translation.py | 2h | 对比三种架构适用场景 |
| Day4：预训练目标原理 | 1. 李沐预训练与微调视频：https://www.bilibili.com/video/BV1Qv411q73c/ <br> 2. 预训练vs微调区别：https://zhuanlan.zhihu.com/p/494812323 | 2h | 整理「预训练打基础」逻辑 |
| Day5：LoRA/Q-LoRA原理 | 1. LoRA论文：https://arxiv.org/abs/2106.09685 <br> 2. Q-LoRA论文：https://arxiv.org/abs/2305.14314 | 2h | 推导低秩矩阵分解公式 |
| Day6：模型规模与算力关系 | 1. Scaling Laws报告：https://arxiv.org/abs/2001.08361 <br> 2. GPU算力对比：https://lambdalabs.com/gpu-benchmarks | 2h | 理解参数量与显存的关系 |
| Day7：架构与术语复盘 | 1. Markdown表格工具：https://www.tablesgenerator.com/markdown_tables <br> 2. 大模型术语字典：https://github.com/glample/tagger/blob/master/dictionaries.md | 2h | 制作架构对比表 |
